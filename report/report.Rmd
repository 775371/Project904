---
title: "POLS 904 Final Project"
subtitle: "Monte Carlo Simulation on Causal Forest"
author: "Jiacheng He"
date: "December 18, 2017"
output: 
  pdf_document: 
    fig_caption: true
---

```{r global, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
setwd("../Project904")
devtools::load_all()
library(ggplot2)
library(dplyr)
library(purrr)
library(pander)
```

## Introduction

In social science, researchers might be interested to estimate the effect of a binary treatment (either treated or not treated). In experimental setting, individuals are randomly assigned into a control group and a treated group. Formally, denote $y_{i}$ as the outcome variable, $W_{i}$ as the treatment assignment variable ($W_{i}=1$ if in the treated group, $W_{i}=0$ if in the control group), and $X_{i}$ as a set of observed covariates (e.g. age, gender, race, eduction, etc). Then the researcher can estimate the classical linear model:

\begin{equation}
Y_{i}=\tau W_{i}+X_{i}\beta+\epsilon_{i}
\end{equation}

Here $\tau=E[Y_{i} | W_{i}=1] - E[Y_{i}|W_{i}=0]$ is intepreted as the average treatment effect (ATE) across all individuals. $X_{i}$ is included into the regression to make sure of unconfoundedness and to reduce the variance of the estimator $\hat{\tau}$. 

But sometimes researchers might want to go beyond ATE, and try to further estimate heterogeneous treatment effect and identify the subgroup of the population who will benefit the most (or least) from the treatment. One approach is to estimate the conditional average treatment effect (CATE) $\tau(X_{i})=E[Y_{i} | X_{i},W_{i}=1] - E[Y_{i}| X_{i},W_{i}=0]$, that is, express the treatment effect $\tau$ as a function of the observed covariates. 

Causal forest developed by Wager and Athey (2017) aims to algorithmically search for the covariate space, identify the subspace where heterogeneity exists, and estimate the CATE in these subspaces. It is very similar to the popular random forest method. Wager and Athey (2017) also derived asymptotic distribution of the causal forest estimator so that statistical inference and hypothesis test become feasible when adopting this forest based method.

In this project, I run Monte Carlo simulation on the causal forest to examine its finite sample perfomance, such as the mean squared error (MSE) and the confidence interval coverage rate. 


## Model Framework 

Consider a simple additive model. Define $\tau(X_{i})=E[Y_{i} | X_{i}, W_{i}=1] - E[Y_{i} | X_{i}, W_{i}=0]$ We will have such relationship:

$$ E[Y_{i} | X_{i}, W_{i}=1] =  E[Y_{i} | X_{i}, W_{i}=0]+W_{i}\cdot \tau(X_{i})$$

With some derivation, we will have such relationship:

$$Y_{i}=m(X_{i})+\frac{W_{i}}{2}\tau(X_{i})+\frac{1-W_{i}}{2}\tau(X_{i})+\epsilon_{i}$$

where


$$
\begin{aligned}
\tau(X_{i})&=E[Y_{i} | X_{i}, W_{i}=1] - E[Y_{i} | X_{i}, W_{i}=0] \\
m(X_{i})&=E[Y_{i} | X_{i}] \\
e(X_{i})&=E[W_{i} | X_{i}]
\end{aligned}
$$

Both $\tau(\cdot)$, $m(\cdot)$, and $e(\cdot)$ are nonparametric functions of the observed covariates $X_{i}$.

There are several challenges in nonparametrically estimate the CATE function $\tau(X_{i})$. First, in real world application, we never observe the true individual treatment effect $\tau_{i}$. At each moment, an individual is either in the treated status or in the non-treated status, so we never know what would have happened to the individual if the individual would have shifted his/her status. This is the fundamental problem in causal inference. As a result of the absence of the true $\tau_{i}$, we can not perform cross validation, which is the routine in predictive machine learning.

Second, the existence of non-constant $m(\cdot)$ and $e(\cdot)$ will tend to confound our estimation, as I showed in the final presentation.


## Causal Forest

The estimation algorithm of the causal forest is very closed to the random forest. There are two major divergence:

1. When growing each tree in the causal forest, we place the split at the point $\tilde{x_{i}}$, which maximizes the difference of $\hat{E}[Y_{i} | X_{i}=x_{i}, W_{i}=1] - \hat{E}[Y_{i} | X_{i}=x_{i}, W_{i}=0]$ ($\hat{\tau}$) across the two sides of $\tilde{x_{i}}$. While in the case of random forest we place the split based on $\hat{E}[Y_{i} | X_{i}=x_{i}]$ ($\hat{y}$).

2. When growing each tree, we use half of the training sample to do Step 1 above (placing split, identifying heterogenity covariate subspace), and use the other half of the training sample to calculate the $\hat{\tau}$ (estimation of the CATE in that subspace). Wager and Athey (2017) refer to this criterion as "honest splitting". 

Also, similar to random forest, in causal forest algorithm it is not necessary to implement regularization or pruning. 


## Simulation Setup

In the Monte Carlo simulation experiment, I am interested to see how the algorithm performs as sample size and number of covariate change, under two different senarios: 1. constant treatment effect; 2. heterogeneous treatment effect. I set up two data generating processes (DGP) as:

DGP1 (constant $\tau$)

$$
\begin{aligned}
\tau(X_{i})&=0  \\
e(X_{i})&=(1+f_{beta}^{2,4}(X_{1i}))/4 \\
m(X_{i})&=2X_{1i}-1 
\end{aligned}
$$

where $f_{beta}^{2,4}(\cdot)$ is the density function of Beta distribution with shape parameters 2 and 4.

DGP2 (heterogeneous $\tau$)
$$
\begin{aligned}
\tau(X_{i})&=1+\frac{1}{(1+e^{-20(X_{1i}-1/3)})(1+e^{-20(X_{2i}-1/3)})} \\  
e(X_{i})&=0.5  \\
m(X_{i})&=0  \\
\end{aligned}
$$
Training causal forests also requires setting up tuning parameters, the same as when we train random forests. In this project, I also try to vary different tuning paramters and evaluate the performance of these trained models. The tuning parameters I try are as follows: (1) sample fraction used in growing each tree; (2) covariates used in growing each tree; (3) Number of trees to build the forest; (4) Minimun number of observations in each terminal leaf; (5) Regularization parameter $\lambda$. 

Please keep in mind that cross validation is feasible in treatment effect estimation. So in practice there is no general guidance to select these tuning parameters in the training stage. Evaluation of choices of tuning parameters is only possible when we assume a data generating process and hence know the true $\tau_{i}$ in Monte Carlo simulation. And we can only implement the evaluation in the test set.

I draw $X_{i}$ ~ $U(0,1)^{d}$, $W_{i}$ ~ $binom(1,e(X_{i}))$, $\epsilon_{i}$ ~ $N(0,1)$. ($d$ is the number of covariates). Then I train the causal forest model on a training set, and evaluate the trained model on a test set with 100 data points. For each senario, I replicate it for 100 times. For each replication I generate a new training set, while the test set is invariant for all replications. Then I plot the box plot of the MSE and 95% confidence interval coverage rate.



## Result

### Sample Size and Number of Covariates

First, I will look at how the performance of the causal forest respond to the change of sample size $n$ and number of covariates $d$:

1. Fix $d=10$, try $n=100, 500, 1000, 2000, 5000$;

2. Fix $n=1000$, try $d=2, 4, 10, 20, 40$;


```{r, echo=FALSE}
n_y_scale_MSE <- c(0, 0.91)
n_y_scale_coverage <- c(0.25, 1.0)

d_y_scale_MSE <- c(0, 0.25)
d_y_scale_coverage <- c(0.39, 1.0)

n_y_scale_sigma <- c(0, 1.0)
d_y_scale_sigma <- c(0, 1.26)

load("../output_df/output_n_DGP1.RData")
box_n_DGP1 <- box_plot_final_output(final_output, x_lab = "n", title = "DGP1: constant tau", y_scale_MSE = n_y_scale_MSE, y_scale_coverage = n_y_scale_coverage, y_scale_sigma = n_y_scale_sigma)

load("../output_df/output_d_DGP1.RData")
box_d_DGP1 <- box_plot_final_output(final_output, x_lab = "d", title = "DGP1: constant tau", y_scale_MSE = d_y_scale_MSE, y_scale_coverage = d_y_scale_coverage, y_scale_sigma = d_y_scale_sigma)

load("../output_df/output_n_DGP2.RData")
box_n_DGP2 <- box_plot_final_output(final_output, x_lab = "n", title = "DGP2: heterogeneous tau", y_scale_MSE = n_y_scale_MSE, y_scale_coverage = n_y_scale_coverage, y_scale_sigma = n_y_scale_sigma)

load("../output_df/output_d_DGP2.RData")
box_d_DGP2 <- box_plot_final_output(final_output, x_lab = "d", title = "DGP2: heterogeneous tau", y_scale_MSE = d_y_scale_MSE, y_scale_coverage = d_y_scale_coverage, y_scale_sigma = d_y_scale_sigma)
  
```

```{r fig1, echo=FALSE, fig.cap="\\label{fig:fig1}MSE and Coverage Rate with Different Sample Size n", fig.pos = "h!", out.extra=""}

multiplot(box_n_DGP1$MSE, 
          box_n_DGP1$coverage, 
          box_n_DGP2$MSE, 
          box_n_DGP2$coverage, cols = 2)
  
```


Figure \ref{fig:fig1} displays the MSE and 95\% confidence interval coverage rate. Under the data generating process of constant treatment effect, the MSE decays very fast as sample size $n$ increases. Under the DGP of heterogeneous treatment effect, the MSE is slightly larger and it decays slightly slower. When $n$ is as large as 5000, we achieve considerably small MSE.

Surprisingly, the confidence interval performs very well for the DGP of constant $\tau$. Even in small sample $n=100$, the simulation coverage rate achieves about 95\%. What is interesting is that the confidence interval seems "over-accurate" for DGP1. In the case of $n=5000$, we obtain almost 100\% accuracy that the 95\% confidence interval will always cover the true $\tau$. 

As for the DGP2, we need larger sample for the confidence coverage to converge. When $n=100$, in median the 95\% confidence interval successfully covers the true $\tau$ for only about 60\% of the time. When $n=1000$, the median coverage rate achieves 95\%. However, when $n=5000$, we have the "over-accurate" issue again.

```{r fig2, echo=FALSE, fig.cap="\\label{fig:fig2}Standard Errors with Different Sample Size n", fig.pos = "h!", out.extra=""}

multiplot(box_n_DGP1$se, 
          box_n_DGP2$se, cols = 2)
  
```

In Figure \ref{fig:fig2} I plot the standard errors in the simulation. We can see that the standard errors are pretty stable and have not converged even when $n=5000$. Theoretically, as sample size increases, we should have smaller standard error for our estimator. But it is clearly not the case here. Figure \ref{fig:fig1} has told us that the MSE is shrinking and basically converges when $n=5000$. In other words, the point estimates are very close to the true value when $n=5000$. Therefore, I conjecture that the standard errors are "too large" in large sample, and hence the 95\% confidence intervals are "too wide", which might lead us to the situation that, in hypothesis testing we are less likely to reject than we should.

```{r fig3, echo=FALSE, fig.cap="\\label{fig:fig3}MSE and Coverage Rate with Different Numbers of Covariates d", out.extra=""}

multiplot(box_d_DGP1$MSE, 
          box_d_DGP1$coverage, 
          box_d_DGP2$MSE, 
          box_d_DGP2$coverage, cols = 2)
  
```

In both data generating processes, only the first two covariates $X1$, $X2$ contribute to the $\tau(\cdot)$ function. Therefore, adding extra covariates is purely adding "noise" to the causal forest algorithm. We should expect the variance of the heterogenous effect estimates increases as $d$ increases. 

The simulation results of DGP2 with heterogeneous treatment effect meet our expectation. Although for resaons when the number of the covariates increases from 2 to 10, the MSE decreases a little bit, the MSE does not keep decreasring when go beyond 10. In fact, the variance of the MSE increases when we increase the number of the noise covariates from 10 to 40. The same happens to the confidence interval coverage rate. Since the sample size is defaulted to 1000, we already achieve a pretty good median coverage rate. As we have more noise covariates, the coverage rate becomes more and more unstable. When $d=40$ (that is, most of the observed covariates are not predictive to $\tau$ at all), in some test data points we will have very small (say, only 40\%) coverage rate by chance.

However, this is not the case for DGP1. As we see in Figure \ref{fig:fig3}, the MSE is shrinking when the number of noise covariates increases. And again we achieve almost 100\% accuracy in the 95\% confidence intervals when $d=40$. This result is puzzling.

```{r fig4, echo=FALSE, fig.cap="\\label{fig:fig4}Standard Errors with Different Numbers of Covariates d"}

multiplot(box_d_DGP1$se, 
          box_d_DGP2$se, cols = 2)
  
```



### Tuning Parameters

I try varying five tuning parameters, one at a time. The simulation setting is default to: DGP2; $n_train=1000$; $d=10$; $n_test=100$.

These are the tuning parameters I test:

1. Sample fraction used in each tree training; (default 0.5)
2. Covariates used in each tree training; (default $\frac{2}{3}d$)
3. Number of trees; (default 2000)
4. Minimun # observations in each terminal node; (default NULL)
5. Regularization parameter $\lambda$; (default 0)

In this section I only report the median MSE and median coverage rate for each senario. The simulation results are as follows:

The causal_forest() function in the grf package does not allow me to set the sample.fraction parameter smaller than 0.5. So I try to vary the sample.fraction from 0.1 to 0.5. As we see, perhaps we should stick to the default value 0.5.

I Try sample fraction $s = 0.1, 0.2, 0.3, 0.4, 0.5$

```{r results='asis', echo=FALSE, }
load("../output_df/output_s.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(s = param) %>% 
  group_by(s) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "MSE and Coverage Rate with Different Sample Fraction s")
```


In causal forest we also use a subset of sample and subset of covariates to grow each tree. The default value of subsetting covariates is $\frac{2}{3}d$. In this case, it is 7. As we see, 7 covariates do give us the best performance, in terms of both MSE and coverage rate.

```{r results='asis', echo=FALSE}
load("../output_df/output_t.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(t = param) %>% 
  group_by(t) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "MSE and Coverage Rate with Different Number of Training Covariates t")
```

Intuitively, the more trees we train, the more variance we will average out. As number of trees increase, we do get smaller and smaller MSE as we tarin more trees. However, the gain is only marginal after we go beyond 1000 trees. But the computer training time is basically linear in the number of training trees. On the other hand, the coverage rate also decreases when we train more trees. Thus it is not clear whether it is worthwhile to train more than 2000 trees.

```{r results='asis', echo=FALSE}
load("../output_df/output_b.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(b = param) %>% 
  group_by(b) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "MSE and Coverage Rate with Different Number of Trees b")
```



The default setting of the algorithm does not set up the minimum node size. I try to vary the minimun node size from 0 to 80. When it defualt to 0, we achieve the best coverage rate. So we should stick to the default value 0.

```{r results='asis', echo=FALSE}
load("../output_df/output_size.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(size = param) %>% 
  group_by(size) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "MSE and Coverage Rate with Different Minimum Node Size")
```

Regularization is one of the most prominent features of machine learning algorithms. When we train decision tree, we usually add a regularization term in the optimization stage and perform backward pruning to prevent over-fitting. But when we do ensemble and average out many trees, we usually do not perform regularization. I try to vary the regularization parameter $\lambda$ from 0 (default value) to 10 to see if we need to regularize in causal forest. The results below indicate that we achieve the best coverage rate, as well as second-best MSE. So, the answer is no we do not need regularization. 

```{r results='asis', echo=FALSE}
load("../output_df/output_lambda.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(lambda = param) %>% 
  group_by(lambda) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "MSE and Coverage Rate with Different Regularization Parameter")
```



