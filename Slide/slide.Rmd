---
title: "Monte Carlo Simulation on Causal Forest"
subtitle: ""
author: "Jiacheng He"
date: "December 13, 2017"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
setwd("../Project904")
devtools::load_all()
library(ggplot2)
library(dplyr)
library(pander)
library(purrr)

```


## Introduction

Sometimes we are interested to estimate the heterogeneous treatment effect of a binary treatment on different subgroups. 

e.g. \
Suppose we run a randomized trial to test a new drug. We spilt the subjects into a control group and a treatment group. What's the effect of this drug for adult white males and teenager Asian females respectively?


## Introduction

Wager and Athey (2017) developed causal forest method to predict heterogeneous treatment effect conditional on observed covariates. 
 
In this project, I test the prediction accuracy and confidence interval coverage rate of causal forest via simulation.


## Causal Forest

Model setup

$Y_{i}$: The outcome variable

$W_{i}$: $W_{i}=1$ if individual $i$ receives treatment, $W_{i}=0$ if not treated

$X_{i}$: A vector of covariates

$\tau_{i}$: Individual treatment effect. Never observed.

\begin{center}
$Y_{i}=m(X_{i})+\frac{W_{i}}{2}\tau(X_{i})+\frac{1-W_{i}}{2}\tau(X_{i})+\epsilon_{i}$
\end{center}

$m(X_{i})=E[Y_{i} | X_{i}]$: The conditional mean of outcome

$\tau(X_{i})=E[Y_{i} | X_{i}, W_{i}=1] - E[Y_{i} | X_{i}, W_{i}=0]$: The heterogeneous treatment effect (conditional on covarites $X_{i}$)

$e(X_{i})=E[W_{i} | X_{i}]$: The treatment propensity


## Causal Forest

Goal is to predict $\tau(X_{i})$ (while random forest aims to predict $m(X_{i})$)

Difficulty:\
1. Disentangle $\tau(X_{i})$ from $m(X_{i})$ and $e(X_{i})$ \
2. Cannot perform cross-validation, because we never observe the true $\tau_{i}$ (while in random forest we observe the true $Y_{i}$)


## Algorithm

Similar to random forest

Place a split at point $\tilde{x_{i}}$ which maximize the difference of $\hat{E}[Y_{i} | X_{i}=x_{i}, W_{i}=1] - \hat{E}[Y_{i} | X_{i}=x_{i}, W_{i}=0]$ across the two sides of $\tilde{x_{i}}$

(while random forest maximize the difference of $\hat{E}[Y_{i} | X_{i}=x_{i}]$)



## Simulation Setup

DGP1 (constant $\tau$)

\begin{center}
$\tau(X_{i})=0$

$e(X_{i})=(1+f_{beta}^{2,4}(X_{1i}))/4$

$m(X_{i})=2X_{1i}-1$
\end{center}


DGP2 (heterogeneous $\tau$)
\begin{center}
$\tau(X_{i})=1+\frac{1}{(1+e^{-20(X_{1i}-1/3)})(1+e^{-20(X_{2i}-1/3)})}$

$e(X_{i})=0.5$

$m(X_{i})=0$
\end{center}


## Simulation Setup

1. Draw $X_{i}$ ~ $U(0,1)^{d}$, $W_{i}$ ~ $binom(1,e(X_{i}))$, $\epsilon_{i}$ ~ $N(0,1)$

2. Run the causal forest on a training set, then evaluate the trained model on a test set. ($n_{train}=n_{test}$)

3. For each senario, replicate it for 100 times. Then compute the average MSE and the 0.95 confidence interval coverage rate


## An Example

```{r}
set.seed(904)
DGP_1 <- list(tau = 0,
              e = expression((1 + dbeta(X1, shape1 = 2, shape2 = 4)) / 4),
              m = expression(2 * X1 - 1))
graphical_illustration(DGP_1, d = 10, n_train = 200, n_test = 200) + labs(title = "DGP1, n = 200, d = 10")
```



## Sample Size and Covariate Size

Vary the sample size $n$ and covariate size $d$ to see what happens:

Fix $d=10$, try $n=100, 500, 1000, 2000, 5000$;

Fix $n=1000$, try $d=2, 4, 10, 20, 40$


## Sample Size and Covariate Size

```{r}
n_y_scale_MSE <- c(0, 0.91)
n_y_scale_coverage <- c(0.25, 1.0)

d_y_scale_MSE <- c(0, 0.25)
d_y_scale_coverage <- c(0.39, 1.0)

n_y_scale_sigma <- c(0, 1.0)
d_y_scale_sigma <- c(0, 1.26)

load("../output_df/output_n_DGP1.RData")
box_n_DGP1 <- box_plot_final_output(final_output, x_lab = "n", title = "DGP1: constant tau", y_scale_MSE = n_y_scale_MSE, y_scale_coverage = n_y_scale_coverage, y_scale_sigma = n_y_scale_sigma)

load("../output_df/output_d_DGP1.RData")
box_d_DGP1 <- box_plot_final_output(final_output, x_lab = "d", title = "DGP1: constant tau", y_scale_MSE = d_y_scale_MSE, y_scale_coverage = d_y_scale_coverage, y_scale_sigma = d_y_scale_sigma)

load("../output_df/output_n_DGP2.RData")
box_n_DGP2 <- box_plot_final_output(final_output, x_lab = "n", title = "DGP2: heterogeneous tau", y_scale_MSE = n_y_scale_MSE, y_scale_coverage = n_y_scale_coverage, y_scale_sigma = n_y_scale_sigma)

load("../output_df/output_d_DGP2.RData")
box_d_DGP2 <- box_plot_final_output(final_output, x_lab = "d", title = "DGP2: heterogeneous tau", y_scale_MSE = d_y_scale_MSE, y_scale_coverage = d_y_scale_coverage, y_scale_sigma = d_y_scale_sigma)

multiplot(box_n_DGP1$MSE, 
          box_n_DGP1$coverage, 
          box_n_DGP2$MSE, 
          box_n_DGP2$coverage, cols = 2)
  
```


## Sample Size and Covariate Size

```{r}

multiplot(box_d_DGP1$MSE, 
          box_d_DGP1$coverage, 
          box_d_DGP2$MSE, 
          box_d_DGP2$coverage, cols = 2)
  
```


## Tuning Parameters

I try varying five tuning parameters, one at a time. I use DGP2 and fix $n=1000, d=10$

1. Sample fraction used in each tree training; (default 0.5)
2. Covariates used in each tree training; (default $\frac{2}{3}d$)
3. Number of trees; (default 2000)
4. Minimun # observations in each terminal node; (default NULL)
5. Regularization parameter $\lambda$; (default 0)


## Tuning Parameters

1. Try sample fraction $s = 0.1, 0.2, 0.3, 0.4, 0.5$

```{r results='asis'}
load("../output_df/output_s.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(s = param) %>% 
  group_by(s) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "Different Sample Fraction s")
```


## Tuning Parameters

2. Try # covariates in each tree training $t = 4, 5, 6, 7, 8$

```{r results='asis'}
load("../output_df/output_t.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(t = param) %>% 
  group_by(t) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "Different Number of Training Covariates t")
```


## Tuning Parameters

3. Try # trees $b = 500, 1000, 2000, 4000, 6000$

```{r results='asis'}
load("../output_df/output_b.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(b = param) %>% 
  group_by(b) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "Different Number of Trees b")
```


## Tuning Parameters

4. Try minimun node size = 0, 10, 20, 40, 80

```{r results='asis'}
load("../output_df/output_size.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(size = param) %>% 
  group_by(size) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "Different Minimum Node Size")
```


## Tuning Parameters

5. Try $\lambda = 0.1, 1, 5, 10, 100$

```{r results='asis'}
load("../output_df/output_lambda.RData")
final_output %>% 
  evaluate_cf() %>% 
  reduce(bind_rows) %>%
  rename(lambda = param) %>% 
  group_by(lambda) %>% 
  summarise(MSE = median(MSE, na.rm = TRUE), coverage = median(coverage, na.rm = TRUE)) %>% 
  round(digits = 3) %>% 
  pandoc.table(caption = "Different Regularization Parameter lambda")
```
