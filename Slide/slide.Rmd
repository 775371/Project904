---
title: "POLS 904 Final Project"
subtitle: "Simulation Study on Causal Forest"
author: "Jiacheng He"
date: "December 13, 2017"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
setwd("C:/Users/j774h/OneDrive/POLS904/Project904/Project904")
devtools::load_all()
library(ggplot2)
library(dplyr)
library(pander)

```


## Introduction

Sometimes we are interested to estimate the heterogeneous treatment effect of a binary treatment on different subgroups. 

e.g. \
Suppose we run a randomized trial to test a new drug. We spilt the subjects into a control group and a treatment group. What's the effect of this drug for adult white males and teenager Asian females respectively?


## Introduction

Wager and Athey (2017) developed causal forest method to predict heterogeneous treatment effect conditional on observed covariates. 
 
In this project, I test the prediction accuracy and confidence interval coverage rate of causal forest via simulation.


## Causal Forest

Model setup

$Y_{i}$: The outcome variable

$W_{i}$: $W_{i}=1$ if individual $i$ receives treatment, $W_{i}=0$ if not treated

$X_{i}$: A vector of covariates

$\tau_{i}$: Individual treatment effect. Never observed.

\begin{center}
$Y_{i}=m(X_{i})+\frac{W_{i}}{2}\tau(X_{i})+\frac{1-W_{i}}{2}\tau(X_{i})+\epsilon_{i}$
\end{center}

$m(X_{i})=E[Y_{i} | X_{i}]$: The conditional mean of outcome

$\tau(X_{i})=E[Y_{i} | X_{i}, W_{i}=1] - E[Y_{i} | X_{i}, W_{i}=0]$: The heterogeneous treatment effect (conditional on covarites $X_{i}$)

$e(X_{i})=E[W_{i} | X_{i}]$: The treatment propensity


## Causal Forest

Goal is to predict $\tau(X_{i})$ (while random forest aims to predict $m(X_{i})$)

Difficulty:\
1. Disentangle $\tau(X_{i})$ from $m(X_{i})$ and $e(X_{i})$ \
2. Cannot perform cross-validation, because we never observe the true $\tau_{i}$ (while in random forest we observe the true $Y_{i}$)


## Algorithm

Similar to random forest

Place a split at point $\tilde{x_{i}}$ which maximize the difference of $\hat{E}[Y_{i} | X_{i}=x_{i}, W_{i}=1] - \hat{E}[Y_{i} | X_{i}=x_{i}, W_{i}=0]$ across the two sides of $\tilde{x_{i}}$

(while random forest maximize the difference of $\hat{E}[Y_{i} | X_{i}=x_{i}]$)



## Simulation Setup

DGP1 (constant $\tau$)

\begin{center}
$\tau(X_{i})=0$

$e(X_{i})=(1+f_{beta}^{2,4}(X_{1i}))/4$

$m(X_{i})=2X_{1i}-1$
\end{center}


DGP2 (heterogeneous $\tau$)
\begin{center}
$\tau(X_{i})=1+\frac{1}{(1+e^{-20(X_{1i}-1/3)})(1+e^{-20(X_{2i}-1/3)})}$

$e(X_{i})=0.5$

$m(X_{i})=0$
\end{center}


## Simulation Setup

1. Draw $X_{i}$ ~ $U(0,1)^{d}$, $W_{i}$ ~ $binom(1,e(X_{i}))$, $\epsilon_{i}$ ~ $N(0,1)$

2. Run the causal forest on a training set, then evaluate the trained model on a test set. ($n_{train}=n_{test}$)

3. For each senario, replicate it for 100 times. Then compute the average MSE and the 0.95 confidence interval coverage rate


## An Example

```{r}
set.seed(904)
DGP_1 <- list(tau = 0,
              e = expression((1 + dbeta(X1, shape1 = 2, shape2 = 4)) / 4),
              m = expression(2 * X1 - 1))
graphical_illustration(DGP_1, d = 10, n = 400) + labs(title = "DGP1, n = 200, d = 10")
```



## Sample Size and Covariate Size

Vary the sample size $n$ and covariate size $d$ to see what happens:

Fix $d=10$, try $n=100, 500, 1000, 2000, 5000$;

Fix $n=1000$, try $d=2, 4, 10, 20, 40$


## Sample Size and Covariate Size

```{r}
load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_n_DGP1.RData")
p1 <- output %>% 
  ggplot() + 
  geom_boxplot(aes(factor(n), MSE)) + 
  scale_y_continuous(limit = c(0, 1.1)) +
  labs(x = "n", title = "DGP1: constant tau")
p2 <- output %>% 
  ggplot() + 
  geom_boxplot(aes(factor(n), coverage)) + 
  scale_y_continuous(limit = c(0.25, 1)) +
  labs(x = "n", y = "Coverage Rate", title = "DGP1: constant tau")

load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_d_DGP1.RData")
p5 <- output %>% 
  ggplot() + 
  geom_boxplot(aes(factor(d), MSE)) + 
  scale_y_continuous(limit = c(0, 0.25)) +
  labs(x = "d", title = "DGP1: constant tau")
p6 <- output %>% 
  ggplot() + 
  geom_boxplot(aes(factor(d), coverage)) + 
  scale_y_continuous(limit = c(0.6, 1)) +
  labs(x = "d", y = "Coverage Rate", title = "DGP1: constant tau")

load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_n_DGP2.RData")
p3 <- output %>% 
  ggplot() + 
  geom_boxplot(aes(factor(n), MSE)) + 
    scale_y_continuous(limit = c(0, 1.1)) +
  labs(x = "n", title = "DGP2: heterogeneous tau")
p4 <- output %>% 
  ggplot() + 
  geom_boxplot(aes(factor(n), coverage)) + 
  scale_y_continuous(limit = c(0.25, 1)) +
  labs(x = "n", y = "Coverage Rate", title = "DGP2: heterogeneous tau")

load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_d_DGP2.RData")
p7 <- output %>% 
  ggplot() + 
  geom_boxplot(aes(factor(d), MSE)) + 
  scale_y_continuous(limit = c(0, 0.25)) +
  labs(x = "d", title = "DGP2: heterogeneous tau")
p8 <- output %>% 
  ggplot() + 
  geom_boxplot(aes(factor(d), coverage)) + 
  scale_y_continuous(limit = c(0.6, 1)) +
  labs(x = "d", y = "Coverage Rate", title = "DGP2: heterogeneous tau")

multiplot(p1, p2, p3, p4, cols = 2)
  
```


## Sample Size and Covariate Size

```{r}

multiplot(p5, p6, p7, p8, cols = 2)
  
```


## Tuning Parameters

I try varying five tuning parameters, one at a time. I use DGP2 and fix $n=1000, d=10$

1. Sample fraction used in each tree training; (default 0.5)
2. Covariates used in each tree training; (default $\frac{2}{3}d$)
3. Number of trees; (default 2000)
4. Minimun # observations in each terminal node; (default NULL)
5. Regularization parameter $\lambda$; (default 0)


## Tuning Parameters

1. Try sample fraction $s = 0.1, 0.2, 0.3, 0.4, 0.5$

```{r results='asis'}
load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_s.RData")
output %>% 
  group_by(s) %>% 
  summarise(MSE = median(MSE), coverage = median(coverage)) %>% 
  pandoc.table()
```


## Tuning Parameters

2. Try # covariates in each tree training $t = 4, 5, 6, 7, 8$

```{r results='asis'}
load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_t.RData")
output %>% 
  group_by(t) %>% 
  summarise(MSE = median(MSE), coverage = median(coverage)) %>% 
  pandoc.table()
```


## Tuning Parameters

3. Try # trees $b = 500, 1000, 2000, 4000, 6000$

```{r results='asis'}
load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_b.RData")
output %>% 
  group_by(b) %>% 
  summarise(MSE = median(MSE), coverage = median(coverage)) %>% 
  pandoc.table()
```


## Tuning Parameters

4. Try minimun node size = 0, 10, 20, 40, 80

```{r results='asis'}
load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_size.RData")
output %>% 
  group_by(size) %>% 
  summarise(MSE = median(MSE), coverage = median(coverage)) %>% 
  pandoc.table()
```


## Tuning Parameters

5. Try $\lambda = 0.1, 1, 5, 10, 100$

```{r results='asis'}
load("C:/Users/j774h/OneDrive/POLS904/Project904/output_df/output_lambda.RData")
output %>% 
  group_by(lambda) %>% 
  summarise(MSE = median(MSE), coverage = median(coverage)) %>% 
  pandoc.table()
```
